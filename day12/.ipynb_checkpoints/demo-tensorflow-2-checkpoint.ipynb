{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Tensorflow\n",
    "\n",
    "In TensorFlow, we have to set up the data, variables, placeholders, and model before we\n",
    "tell the program to train and change the variables to improve the predictions. TensorFlow\n",
    "accomplishes this through the computational graphs. These computational graphs are a\n",
    "directed graphs with no recursion, which allows for computational parallelism. We create a\n",
    "loss function for TensorFlow to minimize. TensorFlow accomplishes this by modifying the\n",
    "variables in the computational graph. Tensorflow knows how to modify the variables because\n",
    "it keeps track of the computations in the model and automatically computes the gradients\n",
    "for every variable. Because of this, we can see how easy it can be to make changes and try\n",
    "different data sources.\n",
    "\n",
    "\n",
    "### Declaring tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are the primary data structure that TensorFlow uses to operate on the computational\n",
    "graph. We can declare these tensors as variables and or feed them in as placeholders. First\n",
    "we must know how to create tensors.\n",
    "\n",
    "When we create a tensor and declare it to be a variable, TensorFlow creates several graph\n",
    "structures in our computation graph. It is also important to point out that just by creating\n",
    "a tensor, TensorFlow is not adding anything to the computational graph. TensorFlow does\n",
    "this only after creating available out of the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "row_dim = 100\n",
    "col_dim = 100\n",
    "\n",
    "# Create a zero filled tensor. Use the following:\n",
    "zero_tsr = tf.zeros([row_dim, col_dim])\n",
    "\n",
    "# Create a one filled tensor. Use the following:\n",
    "ones_tsr = tf.ones([row_dim, col_dim])\n",
    "\n",
    "# Create a constant filled tensor. Use the following:\n",
    "filled_tsr = tf.fill([row_dim, col_dim], 42)\n",
    "\n",
    "# Create a tensor out of an existing constant. Use the following:\n",
    "constant_tsr = tf.constant([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors of similar shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We can also initialize variables based on the shape of other tensors, as\n",
    "# follows:\n",
    "zeros_similar = tf.zeros_like(constant_tsr)\n",
    "print(zeros_similar.shape)\n",
    "ones_similar = tf.ones_like(constant_tsr)\n",
    "print(ones_similar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow allows us to specify tensors that contain defined intervals.\n",
    "# The following functions behave very similarly to the range() outputs and\n",
    "# numpy's linspace() outputs. See the following function:\n",
    "\n",
    "linear_tsr = tf.linspace(start=0., stop=1., num=3)\n",
    "# The resulting tensor is the sequence [0.0, 0.5, 1.0] . Note that this\n",
    "# function includes the specified stop value. See the following function:\n",
    "\n",
    "\n",
    "integer_seq_tsr = tf.range(start=6, limit=15, delta=3)\n",
    "# The result is the sequence [6, 9, 12]. Note that this function does not include\n",
    "# the limit value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.5 1. ]\n",
      "[ 6  9 12]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 3.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# print_op = tf.print(linear_tsr)\n",
    "print(sess.run(linear_tsr))\n",
    "print(sess.run(integer_seq_tsr))\n",
    "sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Some tensor we want to print the value of\n",
    "a = tf.constant([1.0, 3.0])\n",
    "\n",
    "# Add print operation\n",
    "a = tf.Print(a, [a], message=\"This is a: \")\n",
    "\n",
    "a.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following generated random numbers are from a uniform distribution:\n",
    "randunif_tsr = tf.random_uniform([row_dim, col_dim],minval=0, maxval=1)\n",
    "\n",
    "# Note that this random uniform distribution draws from the interval that\n",
    "# includes the minval but not the maxval ( minval <= x < maxval ).\n",
    "# To get a tensor with random draws from a normal distribution, as follows:\n",
    "randnorm_tsr = tf.random_normal([row_dim, col_dim],mean=0.0, stddev=1.0)\n",
    "\n",
    "# There are also times when we wish to generate normal random values that\n",
    "# are assured within certain bounds. The truncated_normal() function\n",
    "# always picks normal values within two standard deviations of the specified\n",
    "# mean. See the following:\n",
    "runcnorm_tsr = tf.truncated_normal([row_dim, col_dim],mean=0.0, stddev=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.3976027 , -0.18463625, -0.7123042 , ..., -0.39387956,\n",
       "         0.58614224,  0.3350849 ],\n",
       "       [ 1.2229058 ,  1.014937  , -1.731907  , ...,  0.10917962,\n",
       "        -1.6860263 ,  0.06779347],\n",
       "       [ 0.1381951 , -0.35920018,  0.04388177, ...,  1.289667  ,\n",
       "         0.8832476 , -1.1727388 ],\n",
       "       ...,\n",
       "       [-1.1578246 ,  1.1584009 ,  1.1673744 , ..., -0.27750298,\n",
       "        -1.4496876 ,  0.75408363],\n",
       "       [ 0.4315164 , -0.78856283, -0.02895579, ...,  1.1594352 ,\n",
       "         1.2009617 , -1.2255534 ],\n",
       "       [ 1.3678681 , -0.12290154,  0.08521585, ..., -1.0783877 ,\n",
       "         0.11354323,  1.1957997 ]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runcnorm_tsr.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We might also be interested in randomizing entries of arrays. To accomplish\n",
    "# this, there are two functions that help us: random_shuffle() and\n",
    "# random_crop() . See the following:\n",
    "input_tensor = runcnorm_tsr\n",
    "crop_size = [row_dim//2, col_dim//2]\n",
    "shuffled_output = tf.random_shuffle(input_tensor)\n",
    "cropped_output = tf.random_crop(input_tensor, crop_size)\n",
    "\n",
    "# We will be interested in randomly cropping an image\n",
    "# of size (height, width, 3) where there are three color spectrums. To fix a\n",
    "# dimension in the cropped_output , you must give it the maximum size in\n",
    "# that dimension:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cropped_image = tf.random_crop(my_image, [height/2, width/2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders and variables\n",
    "\n",
    "Variables are the parameters of the algorithm and TensorFlow keeps track of how\n",
    "to change these to optimize the algorithm\n",
    "\n",
    "The main way to create a variable is by using the Variable() function, which takes a tensor\n",
    "as an input and outputs a variable. This is the declaration and we still need to initialize\n",
    "the variable. Initializing is what puts the variable with the corresponding methods on the\n",
    "computational graph. Here is an example of creating and initializing a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_var = tf.Variable(tf.zeros([2,3]))\n",
    "sess = tf.Session()\n",
    "initialize_op = tf.global_variables_initializer ()\n",
    "sess.run(initialize_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(my_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A placeholder is simply a variable that we will assign data to at a later date. It allows us to create our operations and build our computation graph, without needing the data. In TensorFlow terminology, we then feed data into the graph through these placeholders from a feed_dict argument in the session. \n",
    "\n",
    "To put a placeholder in the graph, we must perform at least one operation on the placeholder. We initialize the graph, declare x to be a placeholder, and define y as the identity operation on x , which just returns x . We then create data to feed into the x placeholder and run the identity operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.96056557, 0.503737  ],\n",
       "       [0.2833927 , 0.30986616]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as  np\n",
    "sess = tf.Session()\n",
    "x = tf.placeholder(tf.float32, shape=[2,2])\n",
    "y = tf.identity(x) # Return a tensor with the same shape and contents as input.\n",
    "x_vals = np.random.rand(2,2)\n",
    "sess.run(y, feed_dict={x: x_vals})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the run of the computational graph, we have to tell TensorFlow when to initialize the\n",
    "variables we have created. TensorFlow must be informed about when it can initialize the\n",
    "variables. While each variable has an initializer method, the most common way to do\n",
    "this is to use the helper function, which is global_variables_initializer() .\n",
    "This function creates an operation in the graph that initializes all the variables we have\n",
    "created, as follows:\n",
    "    \n",
    "initializer_op = tf.global_variables_initializer ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating matrices\n",
    "\n",
    "Creating matrices: We can create two-dimensional matrices from numpy arrays or\n",
    "nested lists. We can also use the\n",
    "tensor creation functions and specify a two-dimensional shape for functions such\n",
    "as zeros() , ones() , truncated_normal() , and so on. TensorFlow also allows\n",
    "us to create a diagonal matrix from a one-dimensional array or list with the function\n",
    "diag() , as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "[[-0.28541327  1.4856031  -0.11091355]\n",
      " [ 0.49297258 -1.0647209  -0.9786521 ]]\n",
      "[[5. 5. 5.]\n",
      " [5. 5. 5.]]\n",
      "[[0.3880278  0.77504754]\n",
      " [0.7160107  0.3825178 ]\n",
      " [0.47572005 0.01854134]]\n"
     ]
    }
   ],
   "source": [
    "identity_matrix = tf.diag([1.0, 1.0, 1.0])\n",
    "A = tf.truncated_normal([2, 3])\n",
    "B = tf.fill([2,3], 5.0)\n",
    "C = tf.random_uniform([3,2])\n",
    "D = tf.convert_to_tensor(np.array([[1., 2., 3.],[-3., -7.,-1.],[0., 5., -2.]]))\n",
    "\n",
    "print(sess.run(identity_matrix))\n",
    "print(sess.run(A))\n",
    "print(sess.run(B))\n",
    "print(sess.run(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.8633952 5.014917  5.7586617]\n",
      " [5.1781    6.6690445 3.4166389]]\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[5. 5. 5.]\n",
      " [5. 5. 5.]]\n",
      "[[0.13826811 0.28341222 0.9185896 ]\n",
      " [0.88222086 0.27464986 0.09976423]]\n",
      "-37.99999999999999\n",
      "[[-0.5        -0.5        -0.5       ]\n",
      " [ 0.15789474  0.05263158  0.21052632]\n",
      " [ 0.39473684  0.13157895  0.02631579]]\n",
      "(array([-10.65907521,  -0.22750691,   2.88658212]), array([[ 0.21749542,  0.63250104, -0.74339638],\n",
      "       [ 0.84526515,  0.2587998 ,  0.46749277],\n",
      "       [-0.4880805 ,  0.73004459,  0.47834331]]))\n"
     ]
    }
   ],
   "source": [
    "# Addition and subtraction uses the following:\n",
    "print(sess.run(A+B))\n",
    "print(sess.run(B-B))\n",
    "\n",
    "# Multiplication\n",
    "print(sess.run(tf.matmul(B, identity_matrix)))\n",
    "\n",
    "# Transpose the arguments as follows:\n",
    "print(sess.run(tf.transpose(C)))\n",
    "\n",
    "# determinant of a matrix\n",
    "print(sess.run(tf.matrix_determinant(D)))\n",
    "\n",
    "# inverse of a matrix\n",
    "print(sess.run(tf.matrix_inverse(D)))\n",
    "\n",
    "# For Eigenvalues and eigenvectors, use the following code:\n",
    "print(sess.run(tf.self_adjoint_eig(D)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.75\n",
      "0.0\n",
      "2.0\n",
      "[0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# It is worth mentioning that div() returns the same type as the inputs.\n",
    "# This means it really returns the floor of the division (akin to Python 2) if the inputs\n",
    "# are integers. To return the Python 3 version, which casts integers into floats before\n",
    "# dividing and always returning a float, TensorFlow provides the function truediv()\n",
    "# function, as shown as follows:\n",
    "print(sess.run(tf.div(3,4)))\n",
    "\n",
    "print(sess.run(tf.truediv(3,4)))\n",
    "\n",
    "# If we have floats and want an integer division, we can use the function floordiv() .\n",
    "# Note that this will still return a float, but rounded down to the nearest integer. The\n",
    "# function is shown as follows:\n",
    "print(sess.run(tf.floordiv(3.0,4.0)))\n",
    "\n",
    "# Another important function is mod() . This function returns the remainder after the\n",
    "# division. It is shown as follows:\n",
    "print(sess.run(tf.mod(22.0, 5.0)))\n",
    "\n",
    "# The cross-product between two tensors is achieved by the cross() function.\n",
    "# Remember that the cross-product is only defined for two three-dimensional vectors,\n",
    "# so it only accepts two three-dimensional tensors. The function is shown as follows:\n",
    "print(sess.run(tf.cross([1., 0., 0.], [0., 1., 0.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n"
     ]
    }
   ],
   "source": [
    "### Custom Polynomial\n",
    "# If we wish to add other operations to our graphs that are not listed here, we must create our\n",
    "# own from the preceding functions. Here is an example of an operation not listed previously\n",
    "# that we can add to our graph. We choose to add a custom polynomial function 3x**2 -x + 10:\n",
    "    \n",
    "def custom_polynomial(x):\n",
    "    return(tf.subtract(3 * tf.square(x), x) + 10)\n",
    "print(sess.run(custom_polynomial(11)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  3. 10.]\n"
     ]
    }
   ],
   "source": [
    "# The rectified linear unit, known as ReLU, is the most common and basic way to\n",
    "# introduce a non-linearity into neural networks. This function is just max(0,x) . It is\n",
    "# continuous but not smooth. It appears as follows:\n",
    "print(sess.run(tf.nn.relu([-3., 3., 10.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 3. 6.]\n"
     ]
    }
   ],
   "source": [
    "# There will be times when we wish to cap the linearly increasing part of the preceding\n",
    "# ReLU activation function. We can do this by nesting the max(0,x) function into\n",
    "# a min() function. The implementation that TensorFlow has is called the ReLU6\n",
    "# function. This is defined as min(max(0,x),6). This is a version of the hard-\n",
    "# sigmoid function and is computationally faster, and does not suffer from vanishing\n",
    "# (infinitesimally near zero) or exploding values. \n",
    "print(sess.run(tf.nn.relu6([-3., 3., 10.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26894143 0.5        0.7310586 ]\n"
     ]
    }
   ],
   "source": [
    "# The sigmoid function is the most common continuous and smooth activation\n",
    "# function. It is also called a logistic function and has the form 1/(1+exp(-x)). The\n",
    "# sigmoid is not often used because of the tendency to zero-out the back propagation\n",
    "# terms during training. It appears as follows:\n",
    "print(sess.run(tf.nn.sigmoid([-1., 0., 1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.7615942  0.         0.7615942]\n"
     ]
    }
   ],
   "source": [
    "# Another smooth activation function is the hyper tangent. The hyper tangent function\n",
    "# is very similar to the sigmoid except that instead of having a range between 0 and\n",
    "# 1 , it has a range between -1 and 1 . The function has the form of the ratio of the\n",
    "# hyperbolic sine over the hyperbolic cosine. But another way to write this is ((exp(x)-\n",
    "# exp(-x))/(exp(x)+exp(-x)). It appears as follows:\n",
    "print(sess.run(tf.nn.tanh([-1., 0., 1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5  0.  -0.5]\n"
     ]
    }
   ],
   "source": [
    "# The softsign function also gets used as an activation function. The form of this\n",
    "# function is x/(abs(x) + 1). The softsign function is supposed to be a continuous\n",
    "# approximation to the sign function. It appears as follows:\n",
    "print(sess.run(tf.nn.softsign([-1., 0., -1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31326166 0.6931472  0.31326166]\n"
     ]
    }
   ],
   "source": [
    "# Another function, the softplus , is a smooth version of the ReLU function. The form\n",
    "# of this function is log(exp(x) + 1). It appears as follows:\n",
    "print(sess.run(tf.nn.softplus([-1., 0., -1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.63212055  0.         -0.63212055]\n"
     ]
    }
   ],
   "source": [
    "# The softplus goes to infinity as the input increases whereas\n",
    "# the softsign goes to 1. As the input gets smaller, however, the\n",
    "# softplus approaches zero and the softsign goes to -1.\n",
    "# The Exponential Linear Unit (ELU) is very similar to the softplus function except\n",
    "# that the bottom asymptote is -1 instead of 0 . The form is (exp(x)+1) if x < 0 else x. It\n",
    "# appears as follows:\n",
    "print(sess.run(tf.nn.elu([-1., 0., -1.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations in a computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "9.0\n",
      "15.0\n",
      "21.0\n",
      "27.0\n"
     ]
    }
   ],
   "source": [
    "# First we declare our tensors and placeholders. Here we will create a numpy array to\n",
    "# feed into our operation:\n",
    "import numpy as np\n",
    "x_vals = np.array([1., 3., 5., 7., 9.])\n",
    "x_data = tf.placeholder(tf.float32)\n",
    "m_const = tf.constant(3.)\n",
    "my_product = tf.multiply(x_data, m_const)\n",
    "for x_val in x_vals:\n",
    "    print(sess.run(my_product, feed_dict={x_data: x_val}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layering Nested Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to know how to chain operations together. This will set up layered operations in\n",
    "the computational graph. For a demonstration we will multiply a placeholder by two matrices\n",
    "and then perform addition. We will feed in two matrices in the form of a three-dimensional\n",
    "numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[102.]\n",
      " [ 66.]\n",
      " [ 58.]]\n",
      "[[114.]\n",
      " [ 78.]\n",
      " [ 70.]]\n"
     ]
    }
   ],
   "source": [
    "# First we create the data to feed in and the corresponding placeholder:\n",
    "my_array = np.array([[1., 3., 5., 7., 9.],[-2., 0., 2., 4., 6.],[-6., -3., 0., 3., 6.]])\n",
    "x_vals = np.array([my_array, my_array + 1])\n",
    "x_data = tf.placeholder(tf.float32, shape=(3, 5))\n",
    "\n",
    "# Next we create the constants that we will use for matrix multiplication and addition:\n",
    "m1 = tf.constant([[1.],[0.],[-1.],[2.],[4.]])\n",
    "m2 = tf.constant([[2.]])\n",
    "a1 = tf.constant([[10.]])\n",
    "\n",
    "# Now we declare the operations and add them to the graph:\n",
    "prod1 = tf.matmul(x_data, m1)\n",
    "prod2 = tf.matmul(prod1, m2)\n",
    "add1 = tf.add(prod2, a1)\n",
    "\n",
    "# Finally, we feed the data through our graph:\n",
    "for x_val in x_vals:\n",
    "    print(sess.run(add1, feed_dict={x_data: x_val}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions are very important to machine learning algorithms. They measure the distance\n",
    "between the model outputs and the target (truth) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = tf.linspace(-1., 1., 500)\n",
    "target = tf.constant(0.)\n",
    "\n",
    "# The L2 norm loss is also known as the Euclidean loss function. It is just the square\n",
    "# of the distance to the target. Here we will compute the loss function as if the target\n",
    "# is zero. The L2 norm is a great loss function because it is very curved near the\n",
    "# target and algorithms can use this fact to converge to the target more slowly, the\n",
    "# closer it gets., as follows:\n",
    "l2_y_vals = tf.square(target - x_vals)\n",
    "l2_y_out = sess.run(l2_y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The L1 norm loss is also known as the absolute loss function. Instead of squaring\n",
    "# the difference, we take the absolute value. The L1 norm is better for outliers than\n",
    "# the L2 norm because it is not as steep for larger values. One issue to be aware of\n",
    "# is that the L1 norm is not smooth at the target and this can result in algorithms not\n",
    "# converging well. It appears as follows:\n",
    "l1_y_vals = tf.abs(target - x_vals)\n",
    "l1_y_out = sess.run(l1_y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo-Huber loss is a continuous and smooth approximation to the Huber loss\n",
    "# function. This loss function attempts to take the best of the L1 and L2 norms by\n",
    "# being convex near the target and less steep for extreme values. The form depends on\n",
    "# an extra parameter, delta, which dictates how steep it will be. We will plot two forms,\n",
    "# delta1 = 0.25 and delta2 = 5 to show the difference, as follows:\n",
    "delta1 = tf.constant(0.25)\n",
    "phuber1_y_vals = tf.multiply(tf.square(delta1), tf.sqrt(1. + tf.square((target - x_vals)/delta1)) - 1.)\n",
    "phuber1_y_out = sess.run(phuber1_y_vals)\n",
    "delta2 = tf.constant(5.)\n",
    "phuber2_y_vals = tf.multiply(tf.square(delta2), tf.sqrt(1. + tf.square((target - x_vals)/delta2)) - 1.)\n",
    "phuber2_y_out = sess.run(phuber2_y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss for a binary classfication case is also sometimes referred to as the logistic loss\n",
    "# function. It comes about when we are predicting the two classes 0 or 1 . We wish to\n",
    "# measure a distance from the actual class ( 0 or 1 ) to the predicted value, which is\n",
    "# usually a real number between 0 and 1 . To measure this distance, we can use the\n",
    "# cross entropy formula from information theory, as follows:\n",
    "xentropy_y_vals = - tf.multiply(target, tf.log(x_vals)) - tf.multiply((1. -target), tf.log(1. - x_vals))\n",
    "xentropy_y_out = sess.run(xentropy_y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = tf.linspace(-3., 5., 500)\n",
    "target = tf.constant(1.)\n",
    "targets = tf.fill([500,], 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid cross entropy loss is very similar to the previous loss function\n",
    "# except we transform the x-values by the sigmoid function before we put them in\n",
    "# the cross entropy loss, as follows:\n",
    "xentropy_sigmoid_y_vals = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_vals, labels=targets)\n",
    "xentropy_sigmoid_y_out = sess.run(xentropy_sigmoid_y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted cross entropy loss is a weighted version of the sigmoid cross entropy\n",
    "# loss . We provide a weight on the positive target. For an example, we will weight the\n",
    "# positive target by 0.5, as follows:\n",
    "weight = tf.constant(0.5)\n",
    "xentropy_weighted_y_vals = tf.nn.weighted_cross_entropy_with_logits(x_vals, targets, weight)\n",
    "xentropy_weighted_y_out = sess.run(xentropy_weighted_y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1601256]\n"
     ]
    }
   ],
   "source": [
    "# Softmax cross-entropy loss operates on non-normalized outputs. This function\n",
    "# is used to measure a loss when there is only one target category instead of multiple.\n",
    "# Because of this, the function transforms the outputs into a probability distribution via\n",
    "# the softmax function and then computes the loss function from a true probability\n",
    "# distribution, as follows:\n",
    "unscaled_logits = tf.constant([[1., -3., 10.]])\n",
    "target_dist = tf.constant([[0.1, 0.02, 0.88]])\n",
    "softmax_xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=unscaled_logits, labels=target_dist)\n",
    "print(sess.run(softmax_xentropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics in models\n",
    "\n",
    "![Fig](imgs/img_003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the benefits of using TensorFlow, is that it can keep track of operations and\n",
    "automatically update model variables based on back propagation.\n",
    "\n",
    "Now we will introduce how to change our variables in the model in such a way that a loss\n",
    "function is minimized. We have learned about how to use objects and operations, and create\n",
    "loss functions that will measure the distance between our predictions and targets. Now\n",
    "we just have to tell TensorFlow how to back propagate errors through our computational\n",
    "graph to update the variables and minimize the loss function. This is done via declaring an\n",
    "optimization function. Once we have an optimization function declared, TensorFlow will go\n",
    "through and figure out the back propagation terms for all of our computations in the graph.\n",
    "When we feed data in and minimize the loss function, TensorFlow will modify our variables in\n",
    "the graph accordingly.\n",
    "\n",
    "For this recipe, we will do a very simple regression algorithm. We will sample random numbers\n",
    "from a normal, with mean 1 and standard deviation 0.1 . Then we will run the numbers\n",
    "through one operation, which will be to multiply them by a variable, A. From this, the loss\n",
    "function will be the L2 norm between the output and the target, which will always be the value\n",
    "10 . Theoretically, the best value for A will be the number 10 since our data will have mean 1 .\n",
    "The second example is a very simple binary classification algorithm. Here we will generate\n",
    "100 numbers from two normal distributions, N(-1,1) and N(3,1). All the numbers from N(-1, 1)\n",
    "will be in target class 0 , and all the numbers from N(3, 1) will be in target class 1 . The model\n",
    "to differentiate these numbers will be a sigmoid function of a translation. In other words,\n",
    "the model will be sigmoid (x + A) where A is a variable we will fit. Theoretically, A will be\n",
    "equal to -1 . We arrive at this number because if m1 and m2 are the means of the two normal\n",
    "functions, the value added to them to translate them equidistant to zero will be â€“(m1+m2)/2.\n",
    "We will see how TensorFlow can arrive at that number in the second example.\n",
    "\n",
    "While specifying a good learning rate helps the convergence of algorithms, we\n",
    "must also specify a type of optimization. From the preceding two examples, we are\n",
    "using standard gradient descent. This is implemented with the TensorFlow function\n",
    "GradientDescentOptimizer() ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we start a graph session:\n",
    "sess = tf.Session()\n",
    "\n",
    "# Next we create the data, placeholders, and the A variable:\n",
    "x_vals = np.random.normal(1, 0.1, 100)\n",
    "y_vals = np.repeat(10., 100)\n",
    "x_data = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "A = tf.Variable(tf.random_normal(shape=[1]))\n",
    "\n",
    "# We add the multiplication operation to our graph:\n",
    "my_output = tf.multiply(x_data, A)\n",
    "\n",
    "# Next we add our L2 loss function between the multiplication output and the target\n",
    "# data:\n",
    "loss = tf.square(my_output - y_target)\n",
    "\n",
    "# Before we can run anything, we have to initialize the variables:\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Now we have to declare a way to optimize the variables in our graph. We declare\n",
    "# an optimizer algorithm. Most optimization algorithms need to know how far to step\n",
    "# in each iteration. This distance is controlled by the learning rate. If our learning\n",
    "# rate is too big, our algorithm might overshoot the minimum, but if our learning rate\n",
    "# is too small, out algorithm might take too long to converge; this is related to the\n",
    "# vanishing and exploding gradient problem. The learning rate has a big influence\n",
    "# on convergence and we will discuss this at the end of the section. While here we\n",
    "# use the standard gradient descent algorithm, there are many different optimization\n",
    "# algorithms that operate differently and can do better or worse depending on the\n",
    "# problem. For a great overview of different optimization algorithms, see the paper by\n",
    "# Sebastian Ruder\n",
    "my_opt = tf.train.GradientDescentOptimizer(learning_rate=0.02)\n",
    "train_step = my_opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #25 A = [6.268843]\n",
      "Loss = [9.51196]\n",
      "Step #50 A = [8.726807]\n",
      "Loss = [4.760973]\n",
      "Step #75 A = [9.405051]\n",
      "Loss = [0.49545825]\n",
      "Step #100 A = [9.723302]\n",
      "Loss = [0.43063903]\n"
     ]
    }
   ],
   "source": [
    "# The final step is to loop through our training algorithm and tell TensorFlow to train\n",
    "# many times. We will do this 101 times and print out results every 25th iteration.\n",
    "# To train, we will select a random x and y entry and feed it through the graph.\n",
    "# TensorFlow will automatically compute the loss, and slightly change the A bias to\n",
    "# minimize the loss:\n",
    "for i in range(100):\n",
    "    rand_index = np.random.choice(100)\n",
    "    rand_x = [x_vals[rand_index]]\n",
    "    rand_y = [y_vals[rand_index]]\n",
    "    sess.run(train_step, feed_dict={x_data: rand_x, y_target:\n",
    "    rand_y})\n",
    "    if (i+1)%25==0:\n",
    "        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)))\n",
    "        print('Loss = ' + str(sess.run(loss, feed_dict={x_data:rand_x, y_target: rand_y})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Classification Using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we reset the graph and reinitialize the graph session:\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will create the data from two different normal distributions, N(-1, 1) and N(3,\n",
    "# 1). We will also generate the target labels, placeholders for the data, and the bias\n",
    "# variable, A :\n",
    "x_vals = np.concatenate((np.random.normal(-1, 1, 50), np.random.normal(3, 1, 50)))\n",
    "y_vals = np.concatenate((np.repeat(0., 50), np.repeat(1., 50)))\n",
    "x_data = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "A = tf.Variable(tf.random_normal(mean=10, shape=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we add the translation operation to the graph. Remember that we do not have to\n",
    "# wrap this in a sigmoid function because the loss function will do that for us:\n",
    "my_output = tf.add(x_data, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the specific loss function expects batches of data that have an extra\n",
    "# dimension associated with them (an added dimension which is the batch number),\n",
    "# we will add an extra dimension to the output with the function, expand_dims() In\n",
    "# the next section we will discuss how to use variable sized batches in training. For\n",
    "# now, we will again just use one random data point at a time:\n",
    "my_output_expanded = tf.expand_dims(my_output, 0)\n",
    "y_target_expanded = tf.expand_dims(y_target, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will initialize our one variable, A :\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we declare our loss function. We will use a cross entropy with unscaled\n",
    "# logits that transforms them with a sigmoid function. TensorFlow has this all in\n",
    "# one function for us in the neural network package called nn.sigmoid_cross_\n",
    "# entropy_with_logits() . As stated before, it expects the arguments to have\n",
    "# specific dimensions, so we have to use the expanded outputs and targets accordingly:\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=my_output_expanded, labels=y_target_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just like the regression example, we need to add an optimizer function to the graph\n",
    "# so that TensorFlow knows how to update the bias variable in the graph:\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.05)\n",
    "train_step = my_opt.minimize(xentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #200 A = [4.952703]\n",
      "Loss = [[0.00010638]]\n",
      "Step #400 A = [0.9153072]\n",
      "Loss = [[1.96158]]\n",
      "Step #600 A = [-0.33452868]\n",
      "Loss = [[0.03684165]]\n",
      "Step #800 A = [-0.95826423]\n",
      "Loss = [[0.11930549]]\n",
      "Step #1000 A = [-0.8842804]\n",
      "Loss = [[0.01034033]]\n",
      "Step #1200 A = [-0.95074403]\n",
      "Loss = [[0.03860547]]\n",
      "Step #1400 A = [-1.0176297]\n",
      "Loss = [[0.8862529]]\n"
     ]
    }
   ],
   "source": [
    "# Finally, we loop through a randomly selected data point several hundred times and\n",
    "# update the variable A accordingly. Every 200 iterations, we will print out the value of\n",
    "# A and the loss:\n",
    "for i in range(1400):\n",
    "    rand_index = np.random.choice(100)\n",
    "    rand_x = [x_vals[rand_index]]\n",
    "    rand_y = [y_vals[rand_index]]\n",
    "    sess.run(train_step, feed_dict={x_data: rand_x, y_target:rand_y})\n",
    "    if (i+1)%200==0:\n",
    "        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)))\n",
    "        print('Loss = ' + str(sess.run(xentropy, feed_dict={x_data: rand_x, y_target: rand_y})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "x_vals = np.random.normal(1, 0.1, 100)\n",
    "y_vals = np.repeat(10., 100)\n",
    "batch_size = 20\n",
    "x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "A = tf.Variable(tf.random_normal(shape=[1,1]))\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "my_output = tf.matmul(x_data, A)\n",
    "\n",
    "# Our loss function will change because we have to take the mean of all the L2 losses\n",
    "# of each data point in the batch. We do this by wrapping our prior loss output in\n",
    "# TensorFlow's reduce_mean() function:\n",
    "loss = tf.reduce_mean(tf.square(my_output - y_target))\n",
    "\n",
    "# We declare our optimizer just like we did before:\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.02)\n",
    "train_step = my_opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #5 A = [[1.0341501]]\n",
      "Loss = 81.02026\n",
      "Step #10 A = [[2.695549]]\n",
      "Loss = 53.308228\n",
      "Step #15 A = [[4.0539355]]\n",
      "Loss = 35.09229\n",
      "Step #20 A = [[5.1429167]]\n",
      "Loss = 23.758549\n",
      "Step #25 A = [[6.0233803]]\n",
      "Loss = 14.981732\n",
      "Step #30 A = [[6.735984]]\n",
      "Loss = 11.018876\n",
      "Step #35 A = [[7.320276]]\n",
      "Loss = 7.4279466\n",
      "Step #40 A = [[7.794171]]\n",
      "Loss = 5.4029264\n",
      "Step #45 A = [[8.191958]]\n",
      "Loss = 4.2883687\n",
      "Step #50 A = [[8.525274]]\n",
      "Loss = 4.063573\n",
      "Step #55 A = [[8.777242]]\n",
      "Loss = 1.5996482\n",
      "Step #60 A = [[8.970854]]\n",
      "Loss = 1.5442941\n",
      "Step #65 A = [[9.141471]]\n",
      "Loss = 1.9802049\n",
      "Step #70 A = [[9.270664]]\n",
      "Loss = 2.1499314\n",
      "Step #75 A = [[9.402825]]\n",
      "Loss = 1.4225581\n",
      "Step #80 A = [[9.483356]]\n",
      "Loss = 1.4588206\n",
      "Step #85 A = [[9.547407]]\n",
      "Loss = 0.8699318\n",
      "Step #90 A = [[9.6110115]]\n",
      "Loss = 1.8988101\n",
      "Step #95 A = [[9.6654825]]\n",
      "Loss = 1.326833\n",
      "Step #100 A = [[9.708594]]\n",
      "Loss = 1.4698954\n"
     ]
    }
   ],
   "source": [
    "loss_batch = []\n",
    "for i in range(100):\n",
    "    rand_index = np.random.choice(100, size=batch_size)\n",
    "    rand_x = np.transpose([x_vals[rand_index]])\n",
    "    rand_y = np.transpose([y_vals[rand_index]])\n",
    "\n",
    "    sess.run(train_step, feed_dict={x_data: rand_x, y_target:rand_y})\n",
    "    if (i+1)%5==0:\n",
    "        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)))\n",
    "        temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "        print('Loss = ' + str(temp_loss))\n",
    "        loss_batch.append(temp_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #5 A = [[1.3049095]]\n",
      "Loss = 73.558754\n",
      "Step #10 A = [[2.9057808]]\n",
      "Loss = 48.625046\n",
      "Step #15 A = [[4.169202]]\n",
      "Loss = 34.29421\n",
      "Step #20 A = [[5.247477]]\n",
      "Loss = 19.132982\n",
      "Step #25 A = [[6.1033216]]\n",
      "Loss = 8.445995\n",
      "Step #30 A = [[6.8230753]]\n",
      "Loss = 6.5411787\n",
      "Step #35 A = [[7.3300037]]\n",
      "Loss = 0.4441245\n",
      "Step #40 A = [[7.8764663]]\n",
      "Loss = 8.401382\n",
      "Step #45 A = [[8.345659]]\n",
      "Loss = 3.1986976\n",
      "Step #50 A = [[8.641721]]\n",
      "Loss = 0.0014285392\n",
      "Step #55 A = [[8.991701]]\n",
      "Loss = 2.8010685\n",
      "Step #60 A = [[9.194006]]\n",
      "Loss = 5.4646\n",
      "Step #65 A = [[9.393196]]\n",
      "Loss = 1.1525879\n",
      "Step #70 A = [[9.460414]]\n",
      "Loss = 1.1738914\n",
      "Step #75 A = [[9.449868]]\n",
      "Loss = 0.43927833\n",
      "Step #80 A = [[9.584822]]\n",
      "Loss = 0.5983\n",
      "Step #85 A = [[9.718265]]\n",
      "Loss = 0.4501239\n",
      "Step #90 A = [[9.637835]]\n",
      "Loss = 0.039481293\n",
      "Step #95 A = [[9.698272]]\n",
      "Loss = 0.12296199\n",
      "Step #100 A = [[9.717434]]\n",
      "Loss = 0.26483947\n"
     ]
    }
   ],
   "source": [
    "loss_stochastic = []\n",
    "for i in range(100):\n",
    "    rand_index = np.random.choice(100)\n",
    "    rand_x = np.array([x_vals[rand_index]]).reshape(-1,1)\n",
    "    rand_y = np.array([y_vals[rand_index]]).reshape(-1,1)\n",
    "    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    if (i+1)%5==0:\n",
    "        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)))\n",
    "        temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "        print('Loss = ' + str(temp_loss))\n",
    "        loss_stochastic.append(temp_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import maplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itv",
   "language": "python",
   "name": "itv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
